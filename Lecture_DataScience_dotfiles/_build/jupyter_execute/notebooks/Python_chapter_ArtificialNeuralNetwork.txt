import numpy as np
def create_toy_data(sample_size, std):
    np.random.seed(1234) #毎回同じデータになるように乱数の種を固定しておく 
    x = np.linspace(0, 0.5, sample_size)
    t = np.sin(2*np.pi*x) + np.random.normal(scale=std, size=x.shape)                                                                                                                 
    return x, t

xt,yt = create_toy_data(40,5.e-2) 

###グラフにしてみる
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(10,4))
ax = fig.add_subplot(111)
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.scatter(xt, yt, facecolor="none", edgecolor="b", s=50, label="Data")
ax.legend()
plt.show()
plt.close()

nhl = 8 ## 隠れ層のノードの数を指定 これを増やすほどニューラルネットワークの表現能力が上がる一方、データに過適合しやすくなる(例外あり)

#重み行列W,V(今はベクトル)と、隠れ層でのバイアスbs,出力層でのバイアスを正規乱数で初期化
np.random.seed(1234)  #結果が実行ごとに同じになるよう乱数を固定(バグを見つけやすくする)
W = np.random.normal(0.0,1.0,nhl)
V = np.random.normal(0.0,1.0,nhl)
bs = np.random.normal(0.0,1.0,nhl)
b0 = np.random.normal()

#シグモイド関数: 活性化関数の一つ
def sigmoid(z):
    return 1.0/(1.0+np.exp(-z))

### データとANNの出力間の二乗誤差を計算する関数を作っておく。
def calc_tloss(x,y,tW,tV,tbs,tb0,acf):
    nhl = len(tW)
    s=0.0
    for i in range(len(x)):
        s += (np.dot(tV, acf(tW*x[i]+tbs)) + tb0 - y[i])**2
    return s

ymean = np.mean(yt)
ystd = np.std(yt)
ny = (np.array(yt)-ymean)/ ystd #それぞれのデータを平均をひいて標準偏差で割る

acf = sigmoid #sigmoid関数をacfという名前で使う

print("初期値での二乗誤差",calc_tloss(xt,ny,W,V,bs,b0,acf))

print("データ1個あたりの誤差:", np.sqrt(calc_tloss(xt,ny,W,V,bs,b0,acf)/len(xt))) #データ1個あたりどれほど誤差*があるか *標準化された誤差

xp = np.linspace(0.0,0.5,300)
yp = np.array([np.dot(V, sigmoid(W*xp[i]+bs)) for i in range(len(xp))])

fig = plt.figure(figsize=(10,4))
ax = fig.add_subplot(111)
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.scatter(xt, yt, facecolor="none", edgecolor="b", s=50, label="Data") 
ax.plot(xp,yp*ystd+ymean,label="ANN") #ニューラルネットワークの予測ypは、"標準化された"yの値に従って学習されているので、元のスケールに戻さないといけない。
ax.legend()
plt.show(); plt.close()

def calc_der(x,y,tW,tV,tbs,tb0,acf,acfder):
    tdw = np.zeros(nhl)
    tdv =  np.zeros(nhl)
    tdbs = np.zeros(nhl)
    tdb0 = 0.0
    #以下の勾配の計算は、目的関数が二乗誤差かつ全データでの勾配の和を使用する場合にのみ正しい
    for i in range(len(x)):
        g = np.dot(tV, acf(tW*x[i]+tbs) ) + tb0 - y[i]
        tdb0 += 2.0 * g
        for jth in range(nhl):                    
            tdv[jth]  += 2.0 * g * acf(tW[jth]*x[i]+tbs[jth])
            tdw[jth]  += 2.0 * g * tV[jth] * acfder(tW[jth]*x[i]+tbs[jth]) *x[i]
            tdbs[jth] += 2.0 * g * tV[jth] * acfder(tW[jth]*x[i]+tbs[jth])
    return tdw, tdv, tdbs, tdb0

#シグモイド関数の微分: 勾配の計算を具体的に求めるのに使う
def sigmoid_der(z):
    return np.exp(-z)/ ((1.0+np.exp(-z))**2)

acf = sigmoid
acfder = sigmoid_der #sigmoid関数の微分sigmoid_derをacfderという名前で使う
calc_der(xt,ny,W,V,bs,b0,acf,acfder)

def fitGD(x,y,tW,tV,tbs,tb0,acf,acfder,nepoch,eta,verbose):
    for i in range(nepoch):
        tdw,tdv,tdbs, tdb0 = calc_der(x,y,tW,tV,tbs,tb0,acf,acfder)
        tW = tW - eta * tdw
        tV = tV -eta * tdv
        tbs =  tbs -eta * tdbs
        tb0 = tb0 -eta * tdb0        
        if verbose == 1:
            print(i, "tloss =", calc_tloss(x,y,tW,tV,tbs,tb0,acf))
    return tW,tV,tbs,tb0,tdw,tdv,tdbs, tdb0

nepoch = 2000
acf = sigmoid; acfder=sigmoid_der
verbose=0
eta = 0.01 #学習率(パラメータ更新のスケールを決めるパラメータ)
W,V,bs,b0,dw,dv,dbs,db0=fitGD(xt,ny,W,V,bs,b0,acf,acfder,nepoch,eta,verbose)

print("学習後の二乗誤差",calc_tloss(xt,ny,W,V,bs,b0,acf))

xp = np.linspace(0, 0.5, 500) 
yp = 0.0*xp 
for i in range(len(yp)):
    yp[i] = np.dot(V, sigmoid(W*xp[i]+bs)) + b0 

fig = plt.figure(figsize=(10,4))
ax = fig.add_subplot(111)
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.scatter(xt, yt, facecolor="none", edgecolor="b", s=50, label="Data")
ax.plot(xp,yp*ystd+ymean,label="ANN") ## ニューラルネットワークの出力は標準化した値に対して学習されていることに注意
ax.legend()
plt.show()
plt.close()

np.random.seed(1234)
W = np.random.normal(0.0,1.0,nhl)
V = np.random.normal(0.0,1.0,nhl)
bs = np.random.normal(0.0,1.0,nhl)
b0 = np.random.normal()

nepoch=20 #20回だけ学習の様子を表示
verbose=1 
print("学習前のloss", calc_tloss(xt,ny,W,V,bs,b0,acf))
#学習
W,V,bs,b0,dw,dv,dbs,db0=fitGD(xt,ny,W,V,bs,b0,acf,acfder,nepoch,eta,verbose)

def updateAdam(A,mt,vt,i,beta1,beta2,eps):
    mhat = mt / (1.0-beta1**(i+1))
    vhat = vt / (1.0-beta2**(i+1))
    return mhat / (np.sqrt( vhat )+eps)

def fitAdam(x,y,tW,tV,tbs,tb0,acf,acfder,nepoch,eta,verbose):
    mts = [ np.zeros(nhl), np.zeros(nhl), np.zeros(nhl), np.zeros(1) ]
    vts  = [ np.zeros(nhl), np.zeros(nhl), np.zeros(nhl), np.zeros(1) ]
    ## Adamで使用するパラメータ
    beta1 = 0.9; beta2 = 0.999; eps = 1.e-6
    omb1 = 1.0-beta1; omb2 = 1.0-beta2
    ## 最適化
    for i in range(nepoch):
        tmp = calc_der(x,y,tW,tV,tbs,tb0,acf,acfder)   ### 勾配を計算するところまでは同じ。
        for n,mt in enumerate(mts):
            mts[n] = beta1 * mt + omb1 * tmp[n]
            vts[n]  = beta2 * vts[n] + omb2 * (tmp[n]**2)
        ### 重み・バイアスの更新
        tW += -eta * updateAdam(tW, mts[0],vts[0],i,beta1,beta2,eps)
        tV  += -eta * updateAdam(tV, mts[1],vts[1],i,beta1,beta2,eps)
        tbs += -eta * updateAdam(tbs,mts[2],vts[2],i,beta1,beta2,eps)
        tb0 += -eta *  (mts[3]/(1.0-beta1**(i+1))) / ( np.sqrt( vts[3]/ (1.0-beta2**(i+1))) + eps)
        if verbose and i % 500 == 0:
            print(i, "tloss =", calc_tloss(x,y,tW,tV,tbs,tb0,acf))        
    return tW,tV,tbs,tb0

np.random.seed(1234) ## Gradient descentと同条件でスタートするためseedを固定
W = np.random.normal(0.0,1.0,nhl)
V = np.random.normal(0.0,1.0,nhl)
bs = np.random.normal(0.0,1.0,nhl)
b0 = np.random.normal()

nepoch=2000
verbose=False
eta = 0.05

acf = sigmoid ; acfder =sigmoid_der
print("学習前のloss", calc_tloss(xt,ny,W,V,bs,b0,acf))
W,V,bs,b0=fitAdam(xt,ny,W,V,bs,b0,acf,acfder,nepoch,eta,verbose)
print("学習後のloss", calc_tloss(xt,ny,W,V,bs,b0,acf))

xp = np.linspace(0, 0.5, 500) 
yp = 0.0*xp 
for i in range(len(yp)):
    yp[i] = np.dot(V, sigmoid(W*xp[i]+bs)) + b0 
ytruth = np.sin(2*np.pi*xp)
fig = plt.figure(figsize=(10,4))
ax = fig.add_subplot(111)
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.scatter(xt, yt, facecolor="none", edgecolor="b", s=50, label="Data")
ax.plot(xp,yp*ystd+ymean,color="C01",label="ANN")
ax.plot(xp,ytruth,color="C02",label="Ground Truth")
ax.legend()
plt.show()
plt.close()

def relu(z):
    return (z > 0)* z   

def relu_der(z):
     return (z > 0)*1.0 
     
#いずれも、zが実数値でもnp.array型のベクトルでも対応可能な表式

##適当な区間のxの値を用意する
xp = np.linspace(-10.0,10.0,100)
yp_sigmoid = sigmoid(xp)
yp_relu = relu(xp)

fig = plt.figure(figsize=(10,4))
ax1 = fig.add_subplot(121)
ax1.plot(xp,yp_sigmoid,label="Sigmoid")
ax1.legend()
ax2 = fig.add_subplot(122)
ax2.plot(xp,yp_relu,label="ReLU")
ax2.legend()
plt.show()
plt.close()

xp = np.linspace(-10.0,10.0,100)
yp_sigmoid = sigmoid_der(xp)
yp_relu = relu_der(xp)

fig = plt.figure(figsize=(10,4))
ax1 = fig.add_subplot(121)
ax1.plot(xp,yp_sigmoid,label="Sigmoid")
ax1.legend()
ax2 = fig.add_subplot(122)
ax2.plot(xp,yp_relu,label="ReLU")
ax2.legend()
plt.show()
plt.close()
