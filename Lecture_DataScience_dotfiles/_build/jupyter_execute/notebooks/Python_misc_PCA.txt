from matplotlib import pyplot as plt
import numpy as np

mean = [165, 68]
mvec = [0,0]
cov=np.array([[1,0.8],[0.8,1.0]])
np.random.seed(1234)
x=[]; y=[]; n=200
for i in range(n):
    tx,ty = np.random.multivariate_normal(mvec,cov)
    x += [ mean[0] + 8*tx]
    y += [ mean[1] + 6*ty]
x=np.array(x);y=np.array(y)
xm = np.mean(x);ym = np.mean(y)
xp = np.arange(140,190,1); yp = np.corrcoef(x,y)[0,1] * (xp-xm) + ym

fig = plt.figure()
plt.xlim(140,190); plt.ylim(40,100)
plt.xlabel("height [cm]")
plt.ylabel("weight [kg]")
plt.scatter(x,y)
plt.plot(xp,yp,color="red",linestyle="dotted")
plt.show();plt.close()

from sklearn import datasets
dataset = datasets.load_iris()
 
target_names = dataset.target_names
targets = dataset.target 
feature_names = dataset.feature_names
features = dataset.data

print(target_names)

import pandas as pd
from pandas import DataFrame
df = DataFrame(features, columns = feature_names)
df['target'] = target_names[targets]
df.head()

from sklearn.decomposition import PCA
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import pyplot as plt

# PCA関数の使用 (3次元)
pca = PCA(n_components=3)
pca.fit(features)
res = pca.fit_transform(features)
print("固有値",pca.explained_variance_)
print("固有ベクトル",pca.components_)
#描画
fig = plt.figure(figsize = (4, 4))
ax = Axes3D(fig) 
for label in np.unique(targets):
    p = ax.scatter(res[targets == label, 0],
                   res[targets == label, 1],
                   res[targets == label, 2],
                   marker = 'o', s = 20)
plt.show()
plt.close()

# PCA関数の使用 (2次元)
pca = PCA(n_components=2)
pca.fit(features)
res = pca.fit_transform(features)

#描画
fig = plt.figure(figsize = (4, 4))
plt.xlabel("PC1"); plt.ylabel("PC2")
for label in np.unique(targets):
    plt.scatter(res[targets == label, 0],res[targets == label, 1],label=target_names[label])
plt.legend()
plt.show()
plt.close()

#上のコードと同様 irisデータを取得
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pandas import DataFrame
from sklearn import datasets
dataset = datasets.load_iris()
 
target_names = dataset.target_names
targets = dataset.target 
feature_names = dataset.feature_names
features = dataset.data

df = DataFrame(features, columns = feature_names) 
df.shape
df.head()

N,p=df.shape
#データ行列の定義
X = df.values 

#p=1,2,3,4 各列の平均ベクトル(p×1行列)を定義
mv = np.array([ np.mean(X[:,i]) for i in range(p) ]).reshape(p,1) 

#共分散行列Cの計算
C = np.zeros((p,p)) # p×pのゼロ行列を作成
for i in range(N):
    d = X[i,:].reshape(p,1) - mv 
    C += np.dot(d,d.T) / N 

vals,vecs = np.linalg.eig(C)
print("固有値",vals)
print("固有ベクトル",vecs.T) #固有ベクトルは縦に並んでいることに注意

#共分散行列Cの計算 (不偏分散, sklearnと整合)
C = np.zeros((p,p)) # p×pのゼロ行列を作成
for i in range(N):
    d = X[i,:].reshape(p,1) - mv 
    C += np.dot(d,d.T) / (N-1) #分母 N → N-1
vals,vecs = np.linalg.eig(C)
print(vals)
print(vecs.T)

PC1s = [ ]
for i in range(N):
    x = X[i,:].reshape(p,1) - mv 
    u = vecs[:,0].reshape(p,1)
    PC1 = np.dot(u.T,x)
    PC1s += [PC1]
    #print("i",i, "PC1",PC1)

PC2s = [ ] 
for i in range(N):
    x = X[i,:].reshape(p,1) - mv 
    u = vecs[:,1].reshape(p,1)
    PC2 = np.dot(u.T,x)
    PC2s += [ PC2 ]

cols = ["blue","orange","green"]

fig = plt.figure(figsize = (4,4))
plt.xlabel("PC1"); plt.ylabel("PC2")
for i in range(N):
    x = PC1s[i]
    y = PC2s[i]
    plt.scatter(x,y,color=cols[targets[i]],alpha=0.4)
plt.show()
plt.close()

fig = plt.figure(figsize = (4, 4))
plt.xlabel("PC1"); plt.ylabel("PC2")
for i in range(N):
    x = PC1s[i]
    y = PC2s[i]
    plt.scatter(x,-y,color=cols[targets[i]],alpha=0.4)
plt.show()
plt.close()

%%capture
#from keras.models import Sequential
#from keras.layers import Dense
!pip install ann_visualizer
from ann_visualizer.visualize import ann_viz
from graphviz import Source
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

graph = temp = '''
... digraph G {
... 
...      graph[ fontname = "Helvetica-Oblique",
...             fontsize = 20,
...             label = "",
...             size = "15,30" ];
... 
...     rankdir = LR;
...     splines=false;
...     edge[style=invis];
...     ranksep= 1.4;
...     {
...     node [shape=circle, color=chartreuse, style=filled, fillcolor=chartreuse];
...     x1 [label=<x<sub>1</sub>>];
...     x2 [label=<x<sub>2</sub>>]; 
...     x3 [label=<x<sub>3</sub>>]; 
...     x4 [label=<x<sub>4</sub>>]; 
...     x5 [label=<x<sub>5</sub>>]; 
...     x6 [label=<x<sub>6</sub>>]; 
... }
... {
...     node [shape=circle, color=dodgerblue, style=filled, fillcolor=dodgerblue];
...     a12 [label=<a<sub>1</sub>>];
...     a22 [label=<a<sub>2</sub>>];
...     a32 [label=<a<sub>3</sub>>];
...     a42 [label=<a<sub>4</sub>>];
... }
... {
...     node [shape=circle, color=coral1, style=filled, fillcolor=coral1];
...     O1 [label=<y<sub>1</sub>>];
...     O2 [label=<y<sub>2</sub>>]; 
...     O3 [label=<y<sub>3</sub>>]; 
...     O4 [label=<y<sub>4</sub>>];
...     O5 [label=<y<sub>5</sub>>]; 
...     O6 [label=<y<sub>6</sub>>]; 
... }
...     {
...         rank=same;
...         x1->x2->x3->x4->x5->x6;
...     }
...     {
...         rank=same;
...         a12->a22->a32->a42;
...     }
...     {
...         rank=same;
...         O1->O2->O3->O4->O5->O6;
...     }
...     l0 [shape=plaintext, label="layer 1 (input layer)"];
...     l0->x1;
...     {rank=same; l0;x1};
...     l1 [shape=plaintext, label="layer 2 (hidden layer)"];
...     l1->a12;
...     {rank=same; l1;a12};
...     l3 [shape=plaintext, label="layer 3 (output layer)"];
...     l3->O1;
...     {rank=same; l3;O1};
...     edge[style=solid, tailport=e, headport=w];
...     {x1; x2;x3;x4;x5;x6} -> {a12;a22;a32;a42};
...     {a12;a22;a32;a42} -> {O1,O2,O3,O4,O5,O6};
... }'''

dot = Source(graph)
dot.format = 'png'
dot.render('neural_network_01', view=False) 

im = Image.open("neural_network_01.png")
plt.figure(figsize = (20,10))
im_list = np.asarray(im)
plt.imshow(im_list)
plt.show()
plt.close()
