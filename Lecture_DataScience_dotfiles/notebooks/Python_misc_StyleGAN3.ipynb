{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHyvtTv97lQH"
      },
      "source": [
        "# StyleGAN3+CLIPによる写真生成\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_s8h-ilzHQc"
      },
      "source": [
        "このノートブックでは、StyleGAN3+CLIPを用いた顔写真生成をデモンストレーションする。  \n",
        "*ランタイムタイプはGPUによる実行が推奨  \n",
        "\n",
        "\n",
        "\n",
        "参考:   \n",
        "- [StyleGAN3](https://nvlabs.github.io/stylegan3/)\n",
        "- [GitHub](https://github.com/NVlabs/stylegan3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFcPiKW85XJ"
      },
      "source": [
        "**StyleGAN3及びCLIPのライセンス**\n",
        "\n",
        "- StyleGAN3はGAN(敵対的生成ネットワーク)を用いた生成モデルのひとつで、NVIDIAから[こちらのライセンス](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt)のもと提供されている。  \n",
        "\n",
        "- CLIPはOpenAIによる画像とテキスト(自然言語)の関連性を学習し画像分類を行うモデルでMITライセンスのもと公開されている[こちら](https://github.com/openai/CLIP)\n",
        "\n",
        "以下のコードを実行したりする分には問題はないが、別の用途で用いる際にはライセンスの詳細を参照し遵守すること.\n",
        "なお、このノートブックの作成においてはこちらの[レポジトリ](https://github.com/ouhenio/StyleGAN3-CLIP-notebooks)を参考にした。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdvqJ8Hx9Sxq"
      },
      "source": [
        "## SetUp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PzDuIoMcqfBT"
      },
      "outputs": [],
      "source": [
        "#@markdown **準備** \n",
        "# @markdown このセルを実行して、ダウンロード等の準備を行おう\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import io\n",
        "import os, time, glob\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Zxbhe4uLvF_a"
      },
      "outputs": [],
      "source": [
        "#@markdown **必要な関数の準備:** このセルも実行しておこう\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    if \"drive.google\" in url_or_path:\n",
        "      if \"18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj\" in url_or_path: \n",
        "        basename = \"wikiart-1024-stylegan3-t-17.2Mimg.pkl\"\n",
        "      elif \"14UGDDOusZ9TMb-pOrF0PAjMGVWLSAii1\" in url_or_path:\n",
        "        basename = \"lhq-256-stylegan3-t-25Mimg.pkl\"\n",
        "    else:\n",
        "        basename = os.path.basename(url_or_path)\n",
        "    if os.path.exists(basename):\n",
        "        return basename\n",
        "    else:\n",
        "        if \"drive.google\" not in url_or_path:\n",
        "          !wget -c '{url_or_path}'\n",
        "        else:\n",
        "          path_id = url_or_path.split(\"id=\")[-1]\n",
        "          !gdown --id '{path_id}'\n",
        "        return basename\n",
        "\n",
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    if len(targets) == 1: # Keeps consitent results vs previous method for single objective guidance \n",
        "      return loss(x, targets[0])\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    return torch.stack(distances, dim=-1).sum(dim=-1)  \n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "def embed_url(url):\n",
        "  image = Image.open(fetch(url)).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/32\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "  \n",
        "clip_model = CLIP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_aZvophLZQOw"
      },
      "outputs": [],
      "source": [
        "#@title  { run: \"auto\" }\n",
        "#@markdown **モデルの選択** \n",
        "\n",
        "\n",
        "#@markdown There are 4 pre-trained options to play with:\n",
        "#@markdown - FFHQ: Trained with human faces.　\n",
        "#@markdown - MetFaces: Trained with paintings/portraits of human faces.\n",
        "#@markdown - AFHQv2: Trained with animal faces.\n",
        "#@markdown - Cosplay: Trained by [l4rz](https://twitter.com/l4rz) with cosplayer's faces.\n",
        "#@markdown - Wikiart: Trained by [Justin Pinkney](https://www.justinpinkney.com/) with the Wikiart 1024 dataset.\n",
        "#@markdown - Landscapes: Trained by [Justin Pinkney](https://www.justinpinkney.com/) with the LHQ dataset.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n",
        "\n",
        "Model = 'MetFaces' #@param [\"FFHQ\", \"MetFaces\", \"AFHQv2\", \"cosplay\", \"Wikiart\", \"Landscapes\"]\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "model_name = {\n",
        "    \"FFHQ\": base_url + \"stylegan3-t-ffhqu-1024x1024.pkl\",\n",
        "    \"MetFaces\": base_url + \"stylegan3-r-metfacesu-1024x1024.pkl\",\n",
        "    \"AFHQv2\": base_url + \"stylegan3-t-afhqv2-512x512.pkl\",\n",
        "    \"cosplay\": \"https://l4rz.net/cosplayface-snapshot-stylegan3t-008000.pkl\",\n",
        "    \"Wikiart\": \"https://archive.org/download/wikiart-1024-stylegan3-t-17.2Mimg/wikiart-1024-stylegan3-t-17.2Mimg.pkl\",\n",
        "    \"Landscapes\": \"https://archive.org/download/lhq-256-stylegan3-t-25Mimg/lhq-256-stylegan3-t-25Mimg.pkl\"\n",
        "}\n",
        "\n",
        "network_url = model_name[Model]\n",
        "\n",
        "with open(fetch_model(network_url), 'rb') as fp:\n",
        "  G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "w_stds = G.mapping(zs, None).std(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAAf8jZevsl"
      },
      "source": [
        "## パラメータの指定と実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z1rqpxkmd_wt"
      },
      "outputs": [],
      "source": [
        "#@markdown **Parameters**\n",
        "\n",
        "#@markdown `texts`: Enter here a prompt to guide the image generation. You can enter more than one prompt separated with\n",
        "#@markdown `|`, which will cause the guidance to focus on the different prompts at the same time, allowing to mix and play\n",
        "#@markdown with the generation process.\n",
        "\n",
        "#@markdown `steps`: Number of optimization steps. The more steps, the longer it will try to generate an image relevant to the prompt.\n",
        "\n",
        "#@markdown `seed`: Determines the randomness seed. Using the same seed and prompt should give you similar results at every run.\n",
        "#@markdown Use `-1` for a random seed.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "texts = \"female | smiling | black hair\"#@param {type:\"string\"}\n",
        "steps = 100#@param {type:\"number\"}\n",
        "seed = 1234#@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "if seed == -1:\n",
        "    seed = np.random.randint(0,9e9)\n",
        "    print(f\"Your random seed is: {seed}\")\n",
        "\n",
        "texts = [frase.strip() for frase in texts.split(\"|\") if frase]\n",
        "\n",
        "targets = [clip_model.embed_text(text) for text in texts]\n",
        "\n",
        "picmod = 30\n",
        "\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "  ])\n",
        "\n",
        "def run(timestring):\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  # Init\n",
        "  # Sample 32 inits and choose the one closest to prompt\n",
        "\n",
        "  with torch.no_grad():\n",
        "    qs = []\n",
        "    losses = []\n",
        "    for _ in range(8):\n",
        "      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "      embeds = embed_image(images.add(1).div(2))\n",
        "      loss = prompts_dist_loss(embeds, targets, spherical_dist_loss).mean(0)\n",
        "      i = torch.argmin(loss)\n",
        "      qs.append(q[i])\n",
        "      losses.append(loss[i])\n",
        "    qs = torch.stack(qs)\n",
        "    losses = torch.stack(losses)\n",
        "    # print(losses)\n",
        "    # print(losses.shape, qs.shape)\n",
        "    i = torch.argmin(losses)\n",
        "    q = qs[i].unsqueeze(0).requires_grad_()\n",
        "\n",
        "  # Sampling loop\n",
        "  q_ema = q\n",
        "  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n",
        "  loop = tqdm(range(steps))\n",
        "  for i in loop:\n",
        "    opt.zero_grad()\n",
        "    w = q * w_stds\n",
        "    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
        "    embed = embed_image(image.add(1).div(2))\n",
        "    loss = prompts_dist_loss(embed, targets, spherical_dist_loss).mean()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "    q_ema = q_ema * 0.9 + q * 0.1\n",
        "    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n",
        "\n",
        "    if i % picmod == 0:\n",
        "      display(TF.to_pil_image(tf(image)[0]))\n",
        "      print(f\"Image {i}/{steps} | Current loss: {loss}\")\n",
        "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "    os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n",
        "\n",
        "try:\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  run(timestring)\n",
        "except KeyboardInterrupt:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x13yvcFnPat"
      },
      "source": [
        "Stepごとに指示に近づいている(上手くいった一例)。\n",
        "\n",
        "少しずつフェードアウトしてくところや、背景が髪の毛と変化(同化)していく点は不思議。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dSRDbGRtjOUq",
        "rAlPquBCSVud",
        "h2K-l3Iu0vHS",
        "c17cT0gy2fwF",
        "OtjKhaH451we",
        "N4vEf_Av9hjN"
      ],
      "name": "Python_misc_StyleGAN3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
